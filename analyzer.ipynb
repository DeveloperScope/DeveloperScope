{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from developerscope.analyzer import get_merge_commits_map\n",
    "\n",
    "current_repo_path = Path().parent.resolve()\n",
    "# TARGET_REPO = \"devQ_testData_PythonProject\"\n",
    "TARGET_REPO = 'toy-python-project'\n",
    "repo_path = current_repo_path.parent / TARGET_REPO\n",
    "\n",
    "\n",
    "merge_commits_map, author_mapping = get_merge_commits_map(repo_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Merge branch 'feature/calc_sub' into 'master'\n",
      "1 Merge branch 'feature/strings' into 'master'\n",
      "2 Merge branch 'feature/json-parser' to 'master'\n",
      "3 Merge branch 'feature/flatten' into 'master'\n",
      "4 Merge branch 'feature/front-end' into 'master'\n",
      "5 Merge branch 'feature/api' into 'master'\n",
      "6 Merge branch 'feature/api-v2' into 'master'\n"
     ]
    }
   ],
   "source": [
    "import git\n",
    "git_repo = git.Repo(str(repo_path))\n",
    "\n",
    "all_commits = []\n",
    "for author, commits in merge_commits_map.items():\n",
    "    for commit in commits:\n",
    "        merge_commit = git_repo.commit(commit)\n",
    "        print(len(all_commits), merge_commit.message.split('\\n')[0])\n",
    "        all_commits.append(merge_commit)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2a909b170dab94861bd113059e6c95fe494621b7'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_commit = all_commits[2]\n",
    "target_commit.hexsha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a secure‑code reviewer.\n",
    "\n",
    "You will receive:\n",
    "• the raw `git diff` of a **merge commit**\n",
    "• the *Halstead total volume* for the changed Python files (objective metric)\n",
    "\n",
    "Tasks:\n",
    "1. **Classify** the merge‑request type – choose exactly one from the list.\n",
    "2. **List potential issues** (security, logic, best practice, etc.) with a severity of LOW‑CRITICAL.\n",
    "3. If the diff alone is insufficient, call **get_file_contents** with the exact file‑paths you still need.\n",
    "Return the result strictly as JSON conforming to the MergeRequestAnalysis\n",
    "\n",
    "OR return nothing and call the `get_file_contents` function. Answer with json only if you think its right thing to do.\n",
    "Don't shy and request files as much as you want. Try to make only one call, include all interesting files. But it is up to you to make sequential calls\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT_REVIEW = \"\"\"\n",
    "You are a senior secure‑code *defender* reviewing an *existing* analysis.\n",
    "\n",
    "1. **If** you need more context, call `get_file_contents`.\n",
    "2. **Then** copy the existing analysis but *keep only* issues with severity HIGH\n",
    "   or CRITICAL.\n",
    "3. Adjust `effortEstimate` if the filtered list changes the scope.\n",
    "Output the same `MergeRequestAnalysis` object.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "\n",
    "from developerscope.analyzer import get_current_state_paths\n",
    "\n",
    "\n",
    "\n",
    "def tool_get_file_contents(targer_commit: git.Commit | None = None, files: list[str] = ['example.txt']):\n",
    "    if targer_commit is not None:\n",
    "        files = get_current_state_paths(targer_commit)\n",
    "    return {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"get_file_contents\",\n",
    "    \"description\": \"Function which accepts a list of files in a git repo and produces a their content\",\n",
    "    \"strict\": True,\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"required\": [\n",
    "            \"files\"\n",
    "        ],\n",
    "        \"properties\": {\n",
    "            \"files\": {\n",
    "                \"type\": \"array\",\n",
    "                \"description\": \"List of specific files to read from the git repository\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": files,\n",
    "                    \"description\": \"File name that exists in the git repository\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from developerscope.analyzer import get_prompt_for_merge_commit\n",
    "\n",
    "\n",
    "input_messages = [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": SYSTEM_PROMPT\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": get_prompt_for_merge_commit(target_commit)\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from developerscope._types import MergeRequestAnalysis\n",
    "import json\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "client = AsyncOpenAI()\n",
    "\n",
    "with open('schema.json') as file:\n",
    "    schemaMergeRequest = json.load(file)\n",
    "\n",
    "async def _get_response(input_messages, tools, required_tool: bool | None):\n",
    "    if required_tool:\n",
    "        tool_choice = 'required'\n",
    "    elif required_tool is None:\n",
    "        tool_choice = 'none'\n",
    "    else:\n",
    "        tool_choice = 'auto'\n",
    "    return await client.responses.create(\n",
    "        model=\"gpt-4.1\",\n",
    "        input=input_messages,\n",
    "        text={\n",
    "            \"format\": schemaMergeRequest\n",
    "        },\n",
    "        temperature=0.2,\n",
    "        tools=tools,\n",
    "        tool_choice= tool_choice,\n",
    "        parallel_tool_calls=False\n",
    "    )\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from developerscope.analyzer import get_current_state\n",
    "\n",
    "def call_function(name, args, commit: git.Commit):\n",
    "    if name == \"get_file_contents\":\n",
    "        return get_current_state(commit, args['files'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from developerscope._types import MergeRequestAnalysis\n",
    "import json\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "\n",
    "\n",
    "async def run_chat_with_functions(input_messages, tools, required_tool=True):\n",
    "    max_calls = 3\n",
    "    for i in range(max_calls):\n",
    "        if i == max_calls - 1:\n",
    "            required_tool = None # means forbidden\n",
    "        response = await(_get_response(input_messages, tools, required_tool=required_tool if tools else False))\n",
    "\n",
    "        if response.output[0].type == 'message':\n",
    "            return response.output[0].content[0]\n",
    "        if response.output[0].type == \"function_call\":\n",
    "            tool_call = response.output[0]\n",
    "            input_messages.append(dict(tool_call))\n",
    "            name = tool_call.name\n",
    "            args = json.loads(tool_call.arguments)\n",
    "            print(name, args)\n",
    "            result = call_function(name, args, merge_commit)\n",
    "            input_messages.append({                               \n",
    "                \"type\": \"function_call_output\",\n",
    "                \"call_id\": tool_call.call_id,\n",
    "                \"output\": str(result)\n",
    "            })\n",
    "            required_tool = False\n",
    "\n",
    "    \n",
    "    return response.output[0].content[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [tool_get_file_contents(target_commit), ]\n",
    "# tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_file_contents {'files': ['README.md', 'unsafe_eval.py']}\n",
      "README.md\n",
      "unsafe_eval.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResponseOutputText(annotations=[], text='{\"hiddenReasoning\":\"The new function \\'json_from_string\\' uses Python\\'s eval() to parse user-provided strings as JSON, which is extremely dangerous. The eval() function executes arbitrary code, so if a user provides a malicious string, it could lead to arbitrary code execution. This is a critical security vulnerability. The correct approach would be to use the standard library\\'s json.loads() for parsing JSON safely. The README does not mention this function, so there is no documentation update or warning about the risk.\",\"type\":\"Feature\",\"issues\":[{\"filePath\":\"unsafe_eval.py\",\"line\":\"3\",\"issue\":\"Use of eval() on user input allows arbitrary code execution. This is a critical security vulnerability. Use json.loads() instead for parsing JSON.\",\"level\":\"CRITICAL\"}],\"effortEstimate\":\"Minor\"}', type='output_text')"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = await run_chat_with_functions(input_messages, tools)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('out.json', 'w') as file:\n",
    "    file.write(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_messages = [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": SYSTEM_PROMPT_REVIEW\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": response.text\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_file_contents {'files': ['unsafe_eval.py']}\n",
      "unsafe_eval.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResponseOutputText(annotations=[], text='{\"hiddenReasoning\":\"The use of eval() on user input in \\'json_from_string\\' is a critical security vulnerability, as it allows arbitrary code execution. This should be replaced with a safe alternative like json.loads().\",\"type\":\"Feature\",\"issues\":[{\"filePath\":\"unsafe_eval.py\",\"line\":\"3\",\"issue\":\"Use of eval() on user input allows arbitrary code execution. This is a critical security vulnerability. Use json.loads() instead for parsing JSON.\",\"level\":\"CRITICAL\"}],\"effortEstimate\":\"Minor\"}', type='output_text')"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = await run_chat_with_functions(input_messages, tools)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('out_review.json', 'w') as file:\n",
    "    file.write(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
