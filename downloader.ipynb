{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "current_repo_path = Path().parent.resolve()\n",
    "TARGET_REPO = \"https://github.com/developerscope/codeutils\"\n",
    "repo_path = current_repo_path.parent / TARGET_REPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/alfaInsurance/devQ_testData_pythonProject/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats CRUD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from developerscope._types import RepositoryStats\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "\n",
    "def _extract_repoName_repoPath_statsPath(url: str) -> tuple[str, Path, Path]:\n",
    "    repo_name = url.rstrip(\"/\").split(\"/\")[-1].removesuffix(\".git\")\n",
    "    repo_path = current_repo_path.parent / repo_name\n",
    "    stats_path = Path() / 'out' / f'{repo_name}.json'\n",
    "\n",
    "    return repo_name, repo_path, stats_path\n",
    "\n",
    "def init_repo_stats(url: str) -> RepositoryStats:\n",
    "    repo_name, repo_path, stats_path = _extract_repoName_repoPath_statsPath(url)\n",
    "    if not repo_path.exists():\n",
    "        raise FileNotFoundError(f\"Expected sibling repository at {repo_path}, but it does not exist.\")\n",
    "\n",
    "    \n",
    "    stats_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if not stats_path.exists():\n",
    "        stats = RepositoryStats(\n",
    "            status=\"NEW\",\n",
    "            url=url,\n",
    "            authors=[],\n",
    "        )\n",
    "        with open(stats_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(stats, f, indent=4)\n",
    "        return stats\n",
    "    else:\n",
    "        with open(stats_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        return RepositoryStats(**data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_repo_stats(stats: RepositoryStats):\n",
    "    url = stats['url']\n",
    "    _, _, stats_path = _extract_repoName_repoPath_statsPath(url)\n",
    "    with open(stats_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(stats, f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = init_repo_stats(url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_repo_stats(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from developerscope._types import RepositoryStats, AuthorStats, BranchStats, CommitStatus\n",
    "from developerscope.analyzer import get_merge_commits_map, get_all_branches\n",
    "\n",
    "\n",
    "def extract_repo_commit_stats(stats: RepositoryStats) -> None:\n",
    "    repo_name, repo_path, _ = _extract_repoName_repoPath_statsPath(stats[\"url\"])\n",
    "    \n",
    "    branches = [head.name for head in get_all_branches(repo_path)]\n",
    "    \n",
    "    authors_map: dict[str, AuthorStats] = {}\n",
    "\n",
    "    for branch in branches:\n",
    "        merge_commits_map, author_mapping = get_merge_commits_map(str(repo_path), only_in_branch=branch)\n",
    "\n",
    "        for username, commit_hashes in merge_commits_map.items():\n",
    "            email, name = sorted(author_mapping[username])[0]  # pick first (stable)\n",
    "\n",
    "            if username not in authors_map:\n",
    "                authors_map[username] = {\n",
    "                    \"name\": name,\n",
    "                    \"email\": email,\n",
    "                    \"branches\": []\n",
    "                }\n",
    "\n",
    "            commit_statuses: list[CommitStatus] = [\n",
    "                {\"commitHash\": h, \"status\": \"NEW\"} for h in commit_hashes\n",
    "            ]\n",
    "\n",
    "            authors_map[username][\"branches\"].append({\n",
    "                \"name\": branch,\n",
    "                \"commits\": commit_statuses\n",
    "            })\n",
    "\n",
    "    stats[\"authors\"] = list(authors_map.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "\n",
    "def update_repo_commit_status(hexsha: str, status: Literal[\n",
    "        \"NEW\",\n",
    "        \"PENDING\",\n",
    "        \"DONE\",\n",
    "    ], stats: RepositoryStats):\n",
    "    for author in stats['authors']:\n",
    "        for branch in author['branches']:\n",
    "            for commit in branch['commits']:\n",
    "                if commit['commitHash'] == hexsha:\n",
    "                    commit['status'] = status\n",
    "                    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_repo_stats(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis CRUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import cast\n",
    "from developerscope._types import AuthorsAnalysis, DetailedMergeRequestAnalysis\n",
    "from developerscope.analyzer import extract_username\n",
    "from report_generator import MergeRequestAnalysis\n",
    "\n",
    "\n",
    "def get_author_analysis(stats: RepositoryStats, author: str) -> AuthorsAnalysis:\n",
    "    author = author.lower() \n",
    "    repo_name, _, stats_path = _extract_repoName_repoPath_statsPath(stats[\"url\"])\n",
    "    out_dir_repo = stats_path.parent / repo_name\n",
    "    out_dir_repo.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    author_file = out_dir_repo / f\"{author}.json\"\n",
    "\n",
    "    # find the author in stats\n",
    "    \n",
    "    for authorStats in stats['authors']:\n",
    "        if extract_username(authorStats['email']).lower() == author.lower():\n",
    "            break\n",
    "    else:\n",
    "        raise KeyError(f\"Author '{author}' not found in RepositoryStats\")\n",
    "\n",
    "    # initialize if missing\n",
    "    if not author_file.exists():\n",
    "        placeholder: AuthorsAnalysis = {\n",
    "            \"author\": author,\n",
    "            \"branches\": [\n",
    "                {\n",
    "                    \"branch\": br[\"name\"],\n",
    "                    \"mergeRequests\": []\n",
    "                }\n",
    "                for br in authorStats[\"branches\"]\n",
    "            ]\n",
    "        }\n",
    "        with open(author_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(placeholder, f, indent=4, ensure_ascii=False)\n",
    "        return placeholder\n",
    "\n",
    "    # otherwise load existing\n",
    "    with open(author_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_author_analysis(stats: RepositoryStats, analysis: AuthorsAnalysis):\n",
    "    \"\"\"\n",
    "    Write back the given AuthorsAnalysis to:\n",
    "      out/<repo_name>/authors_analysis/<author>.json\n",
    "    \"\"\"\n",
    "    # assumes you still have your repoâ€stats object in `stats`\n",
    "    repo_name, _, stats_path = _extract_repoName_repoPath_statsPath(stats[\"url\"])\n",
    "    out_dir_repo = stats_path.parent / repo_name\n",
    "    out_dir_repo.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    author_file = out_dir_repo / f\"{analysis['author']}.json\"\n",
    "\n",
    "\n",
    "    author_file = out_dir_repo / f\"{analysis['author']}.json\"\n",
    "    with open(author_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(analysis, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    return cast(AuthorsAnalysis, author_file)\n",
    "\n",
    "\n",
    "def insert_merge_requests(\n",
    "        author_analysis: AuthorsAnalysis, \n",
    "        stats: RepositoryStats,\n",
    "        merge_request: DetailedMergeRequestAnalysis,\n",
    "        ):\n",
    "    for author in stats['authors']:\n",
    "        if extract_username(author[\"email\"]).lower() != author_analysis[\"author\"].lower():\n",
    "            continue\n",
    "\n",
    "        for branch in author['branches']:\n",
    "            for commit in branch['commits']:\n",
    "                if commit[\"commitHash\"] == merge_request[\"commitHash\"]:\n",
    "                    break\n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "        break\n",
    "    else:\n",
    "        raise KeyError('Commit did not find in stats')\n",
    "    \n",
    "    commit[\"status\"] = \"DONE\"\n",
    "    for branch_a in author_analysis['branches']:\n",
    "        if branch_a['branch'] != branch['name']:\n",
    "            continue\n",
    "        branch_a[\"mergeRequests\"].append(merge_request)\n",
    "        return\n",
    "    else:\n",
    "        raise KeyError('No such branch in the author_analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_repo_commit_stats(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "\n",
    "from developerscope._types import CommitMetrics\n",
    "from developerscope.haslted import halstead_effort\n",
    "\n",
    "def get_metrics(commit: git.Commit) -> CommitMetrics:\n",
    "    return {'halstedEffort' : halstead_effort(commit)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_repo = git.Repo(repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from developerscope.gpt import anylyze_commit \n",
    "\n",
    "\n",
    "async def process_commit(commit: git.Commit, stats: RepositoryStats) -> DetailedMergeRequestAnalysis:\n",
    "    # Analyze the commit (asynchronous)\n",
    "    try:\n",
    "        analysis: MergeRequestAnalysis = await anylyze_commit(commit)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "    # Add metrics/details\n",
    "    detailed: DetailedMergeRequestAnalysis = {\n",
    "        **analysis,\n",
    "        'commitHash': commit.hexsha,\n",
    "        'metrics': get_metrics(commit)\n",
    "    }\n",
    "\n",
    "    # Save to author analysis and repo stats\n",
    "    author_username = extract_username(commit.author.email)\n",
    "    author_analysis: AuthorsAnalysis = get_author_analysis(stats, author_username)\n",
    "    \n",
    "    insert_merge_requests(author_analysis, stats, detailed)\n",
    "    save_author_analysis(stats, author_analysis)\n",
    "    save_repo_stats(stats)\n",
    "    print('done')\n",
    "    return detailed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def process_batch(batch: list[git.Commit], stats: RepositoryStats):\n",
    "    return await asyncio.gather(*(process_commit(c, stats) for c in batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<git.Commit \"2a2f65f4b7857dc77eb878367eeb7b581186717d\">,\n",
       " <git.Commit \"59661b198d08946e71a426def0f579ecfa8d2512\">,\n",
       " <git.Commit \"a52c340ca357fce955b78186636b699e905bc2e5\">,\n",
       " <git.Commit \"53bd56569b432c87c27b82ca60c596e751a0cfa0\">,\n",
       " <git.Commit \"764b229319e3fef4ea540a0506dc3be6d9e18c71\">,\n",
       " <git.Commit \"0cac8e3b343f224055f2905bddcf2cbfe40bc90d\">,\n",
       " <git.Commit \"10d52c8242699f0e6d81628c7b12c16ea55cca7f\">,\n",
       " <git.Commit \"cbb9b259fddc5381ef0c787479a2c50012689850\">,\n",
       " <git.Commit \"353119ed2c5a7b870e9c0fa646aeedc37338594b\">,\n",
       " <git.Commit \"53bd56569b432c87c27b82ca60c596e751a0cfa0\">]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch: list[git.Commit] = []\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_file_contents {'files': ['calc_sub.py']}\n",
      "calc_sub.py\n",
      "get_file_contents {'files': ['calc_sub.py']}\n",
      "calc_sub.py\n",
      "done\n",
      "SUCESS: 1\n",
      "get_file_contents {'files': ['string_lower.py', 'string_reverse.py', 'string_upper.py']}\n",
      "string_lower.py\n",
      "string_reverse.py\n",
      "string_upper.py\n",
      "get_file_contents {'files': ['string_lower.py', 'string_upper.py', 'string_reverse.py']}\n",
      "string_lower.py\n",
      "string_reverse.py\n",
      "string_upper.py\n",
      "done\n",
      "SUCESS: 2\n",
      "get_file_contents {'files': ['unsafe_eval.py']}\n",
      "unsafe_eval.py\n",
      "get_file_contents {'files': ['unsafe_eval.py']}\n",
      "unsafe_eval.py\n",
      "done\n",
      "SUCESS: 3\n",
      "get_file_contents {'files': ['list_flatten.py']}\n",
      "list_flatten.py\n",
      "get_file_contents {'files': ['list_flatten.py']}\n",
      "list_flatten.py\n",
      "done\n",
      "SUCESS: 4\n",
      "get_file_contents {'files': ['admin.html', 'index.html']}\n",
      "admin.html\n",
      "index.html\n",
      "get_file_contents {'files': ['index.html', 'admin.html']}\n",
      "admin.html\n",
      "index.html\n",
      "done\n",
      "SUCESS: 5\n",
      "get_file_contents {'files': ['api.py']}\n",
      "api.py\n",
      "get_file_contents {'files': ['api.py']}\n",
      "api.py\n",
      "done\n",
      "SUCESS: 6\n",
      "get_file_contents {'files': ['api.py']}\n",
      "api.py\n",
      "get_file_contents {'files': ['api.py']}\n",
      "api.py\n",
      "done\n",
      "SUCESS: 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7, 0, 7)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs = 0\n",
    "haslted = 0\n",
    "success = 0\n",
    "for author in stats['authors']:\n",
    "    for branch in author['branches']:\n",
    "        for commit in branch['commits']:\n",
    "            jobs += 1\n",
    "            if commit['status'] != 'NEW':\n",
    "                continue\n",
    "            git_commit = git_repo.commit(commit['commitHash'])\n",
    "            # if halstead_effort(git_commit) > 0:\n",
    "            haslted += 1\n",
    "            batch.append(git_commit)\n",
    "            # if haslted <= batch_size:\n",
    "            result = await process_batch(batch, stats)\n",
    "            success += len([x for x in result if isinstance(x, dict)])\n",
    "            print('SUCESS:', success)\n",
    "            batch.clear()\n",
    "            haslted = 0\n",
    "        else:\n",
    "            continue\n",
    "        break\n",
    "    else:\n",
    "        continue\n",
    "    break\n",
    "\n",
    "jobs, haslted, success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await process_batch(batch, stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1435.41"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "halstead_effort(git_repo.commit('e653be919f10f67e77ccdf9ae877ee73151ae06e'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
